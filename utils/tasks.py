"""
Utilitaire pour r√©cup√©rer les donn√©es (datasets) des plateformes Open Data

Les donn√©es sont r√©cup√©r√©es via les API de data.economie.gouv.fr

Les donn√©es ainsi r√©cup√©r√©es sont fusionn√©es et servent √† peupler la base de donn√©es.
"""

import json
import os

import requests
from dotenv import load_dotenv

from application.handlers import find_platform_from_url, upsert_dataset, create_platform
from exceptions import DatasetHasNotChanged, DatasetUnreachableError
from logger import logger
from settings import BASE_DIR, ENV_PATH, app

load_dotenv(ENV_PATH)

API_KEY = os.environ["DATA_ECO_API_KEY"]
HEADERS = {"Authorization": f"Apikey {API_KEY}"}
OUTPUT_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(OUTPUT_DIR, exist_ok=True)

API_ENDPOINTS = [
    {
        "url": "https://data.economie.gouv.fr/api/automation/v1.0/datasets",
        "params": {"limit": 1000},
        "filename": "data-eco-automation.json",
    },
    {
        "url": "https://data.economie.gouv.fr/api/explore/v2.1/monitoring/datasets/ods-datasets-monitoring/exports/json",
        "params": {
            "where": 'domain_id="opendatamef"',
            "order_by": "modified DESC",
            "limit": 1000,
        },
        "filename": "data-eco-monitoring.json",
    },
    {
        "url": "https://data.economie.gouv.fr/api/explore/v2.1/catalog/exports/json",
        "params": {"limit": 1000},
        "filename": "data-eco-catalog.json",
    },
]


# Fonctions utilitaires
def fetch_and_save_data(url: str, params: dict, filename: str) -> dict:
    """R√©cup√®re des donn√©es via API et les sauvegarde dans un fichier JSON"""
    try:
        response = requests.get(url, headers=HEADERS, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        # Gestion des diff√©rents formats de r√©ponse
        if isinstance(data, list):
            results = data  # R√©ponse directe sous forme de liste
        else:
            results = data.get("results", data)  # R√©ponse avec cl√© "results"

        with open(os.path.join(OUTPUT_DIR, filename), "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        logger.info(f"‚úÖ {filename} - {len(results)} √©l√©ments sauvegard√©s")
        return results
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Erreur pour {filename}: {e}")
        return []


def load_json_by_id(filename: str, key: str = "dataset_id") -> dict:
    """Charge un fichier JSON et cr√©e un dictionnaire index√© par une cl√©"""
    filepath = os.path.join(OUTPUT_DIR, filename)
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            return {item[key]: item for item in data}
    except FileNotFoundError:
        logger.error(f"‚ö†Ô∏è Fichier {filename} non trouv√©")
        return {}


def merge_datasets(*sources: dict) -> list:
    """Fusionne plusieurs sources de donn√©es par ID de dataset"""
    all_ids = set().union(*[source.keys() for source in sources])
    merged = []

    for dataset_id in all_ids:
        merged_data = {}
        for source in sources:
            if dataset_id in source:
                merged_data.update(source[dataset_id])

        if merged_data:
            merged.append(merged_data)
    return merged


def merge_data_eco_datasets():
    datasets = {}
    for endpoint in API_ENDPOINTS:
        data = fetch_and_save_data(
            endpoint["url"], endpoint["params"], endpoint["filename"]
        )
        datasets[endpoint["filename"]] = data

    automation = load_json_by_id("data-eco-automation.json", "dataset_id")
    monitoring = load_json_by_id("data-eco-monitoring.json", "dataset_id")
    catalog = load_json_by_id("data-eco-catalog.json", "dataset_id")

    merged_data = merge_datasets(automation, monitoring, catalog)
    logger.info(f"üîÄ {len(merged_data)} datasets fusionn√©s")

    with open(
            os.path.join(OUTPUT_DIR, "data-eco.json"), "w", encoding="utf-8"
    ) as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    logger.info("üíæ Fichier final data-eco.json sauvegard√©")


def process_data_gouv():
    organization = os.environ["DATA_GOUV_ORGANIZATION"]
    url = f"http://www.data.gouv.fr/api/1/datasets/"
    params = {"organization": organization, "page_size": 1000}

    response = requests.get(url, params=params)
    # Robustifier la lecture du JSON retourn√© par data.gouv :
    # - certaines r√©ponses contiennent une cl√© 'data' ; d'autres retournent
    #   directement une liste. On g√®re les deux cas et on journalise le
    #   contenu inattendu pour d√©bogage.
    try:
        payload = response.json()
    except Exception as e:
        logger.error(f"‚ùå Impossible de d√©coder la r√©ponse data.gouv : {e}")
        return

    if isinstance(payload, dict) and "data" in payload:
        # parfois 'data' peut √™tre None; coerce en liste vide si n√©cessaire
        data = payload["data"] or []
    elif isinstance(payload, list):
        data = payload
    else:
        # Cas inattendu : sauvegarder la r√©ponse brute pour analyse
        logger.error("‚ö†Ô∏è R√©ponse data.gouv inattendue, sauvegarde pour inspection")
        with open(os.path.join(OUTPUT_DIR, "data-gouv-raw.json"), "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
        # Essayer d'extraire une liste depuis 'results' si pr√©sente,
        # avec fallback sur liste vide pour √©viter TypeError en aval
        data = payload.get("results", []) if isinstance(payload, dict) else []

    with open(os.path.join(OUTPUT_DIR, "data-gouv.json"), "w", encoding="utf-8") as file:
        text = json.dumps(data, ensure_ascii=False, indent=2, sort_keys=True)
        file.write(text)

    with open(os.path.join(OUTPUT_DIR, "data-gouv.json"), "r") as file:
        data = json.load(file)
        for dataset in data:
            platform = find_platform_from_url(app=app, url=dataset["page"])
            try:
                upsert_dataset(app=app, platform=platform, dataset=dataset)
            except DatasetHasNotChanged as e:
                logger.error(f' - {dataset["dataset_id"]} - {e}')
            except DatasetUnreachableError:
                pass


def process_data_eco():
    merge_data_eco_datasets()
    with open(os.path.join(OUTPUT_DIR, "data-eco.json"), "r") as file:
        data = json.load(file)
        for dataset in data:
            # Chercher la platform existante par domaine. Si elle n'existe pas,
            # on la cr√©e automatiquement avec des valeurs raisonnables afin
            # que l'ingestion puisse s'ex√©cuter de fa√ßon idempotente.
            # Cela √©vite que l'ingestion soit silencieusement ignor√©e parce
            # que la plateforme n'a pas √©t√© enregistr√©e manuellement.
            platform = find_platform_from_url(app=app, url="https://data.economie.gouv.fr")
            try:
                if platform is None:
                    logger.info("Platform introuvable pour data.economie.gouv.fr ‚Äî cr√©ation automatique")
                    platform_payload = {
                        "name": "Data Economie",
                        "slug": "data-economie",
                        "organization_id": "opendatamef",
                        "type": "opendatasoft",
                        "url": "https://data.economie.gouv.fr",
                        "key": os.environ.get("DATA_ECO_API_KEY"),
                    }
                    try:
                        # create_platform g√®re l'insert via l'application (DDD)
                        create_platform(app=app, data=platform_payload)
                        # recharger la platform fra√Æchement cr√©√©e
                        platform = find_platform_from_url(app=app, url=platform_payload["url"])
                    except Exception as ce:
                        logger.error(f"Erreur lors de la cr√©ation automatique de la platform: {ce}")
                        # si on n'a pas pu cr√©er la platform, on skip ce dataset
                        continue

                upsert_dataset(app=app, platform=platform, dataset=dataset)
            except Exception as e:
                logger.debug(f'OPENDATASOFT - {dataset.get("dataset_id", "?" )} - {e}')


if __name__ == "__main__":
    process_data_eco()
    process_data_gouv()
